# 监督学习（supervised learning）

学习一个模型，使模型能够对任意给定的输入，对其相应的输出做出一个好的预测。利用训练数据集学习一个模型，再用模型对测试样本集进行预测。

根据已知关系训练得到的一个最优模型，在监督学习中，训练数据既有特征又有标签，**通过训练，使得机器能找到特征和标签之间的联系，然后再面对没有标签的数据时可以判断出标签。**在监督学习的范畴中，又可以**划分成回归和分类。**



![image-20221005091625524](https://raw.githubusercontent.com/kaikaihit/kaiPic/main/image-20221005091625524.png)



## 感知机（Perceptron Model）

**二分类模型**，输入为实例的特征向量，输出为实例的类别。根据输入的特征向量，感知机将特征空间进行划分，形成正负两类。

感知机数学模型，给定输入和输出的情况下，找到一个权重W，b，使得损失函数L越小越好。

使用**梯度下降算法**，计算机求解数学问题时常用的方法之一，目的主要是通过逐步迭代找到目标函数的最小值（极小值）或者收敛到最小值（或极小值）



<img src="https://raw.githubusercontent.com/kaikaihit/kaiPic/main/image-20221007103457761.png" alt="image-20221007103457761" style="zoom:50%;" />

**问题：求解的是局部最优值，得到的结果不一定是全局最优值。**

- 批量梯度下降法
- 小批量梯度下降法
- 随机梯度下降法



### 批量梯度下降法BGD（Batch Gradient Descent）

针对整个数据集，对所有样本计算，求解梯度的方向。优点：全局最优解，易于并行实现；缺点：当样本数据多时，耗费资源，计算速度慢。



### 小批量梯度下降法MBGD（mini-batch Gradient Descent）

把数据分成若干批，按批来更新参数。一批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。优点：减少了计算的开销量，降低了随机性。



### 随机梯度下降法SGD（stochastic gradient descent）

简单且有效的方法，用于支持向量机，逻辑回归等凸损失函数下的线性分类器的学习。既可用于分类计算，也可用于回归计算。

1.分类

```python
sklearn.linear_model.SGDClassifier
```

2.回归

```python
sklearn.linear_model
```



## K-近邻算法KNN（K-Nearest Neighbor）

经典和最简单的有监督学习方法之一。K-近邻算法是最简单的分类器，没有显示的学习过程或训练过程，是懒惰学习（lazy learning）。当对数据的分布只有很少或者没有任何先验知识时，K近邻算法是一个不错的选择。

当对测试样本进行分类时，首先通过扫描训练样本集，找到与该测试样本最相似的个训练样本，根据这个样本的类别进行投票确定测试样本的类别。也可以通过个样本与测试样本的相似程度进行加权投票。如果需要以测试样本对应每类的概率的形式输出，可以通过个样本中不同类别的样本数量分布来进行估计。

![img](https://raw.githubusercontent.com/kaikaihit/kaiPic/main/e211536cca024535725c4c4a72353d03.png)

K的取值很重要，如果k=3，在其最近的3个样本中红色三角形数量最多，绿圆属于红色三角形类别；如果k=5，在其最近的5个样本中蓝色矩形数量最多，绿圆属于蓝色矩形类别，k的取值很重要。

K近邻算法中一个重要问题是计算样本之间的距离，确定训练样本中哪些样本与测试样本更加接近。

在实际应用中，需要根据应用的场景和数据本身的特点来选择距离计算方法。当已有的距离方法不能满足实际应用需求时，需要针对性地提出适合具体问题地距离度量方法。
$$
{L_p}({x_i},{x_j}) = {(\sum\limits_{l = 1}^n {|x_i^{(l)} - x_j^{(l)}{|^p}} )^{\frac{1}{p}}}
$$


- p=2时，为欧式距离。
- p=1时，为曼哈顿距离。
- p=∞，为各个坐标距离的最大值



## 朴素贝叶斯法

基于贝叶斯定理与特征条件独立假设的分类方法。基于特征条件独立假设学习输入输出的联合概率分布；然后基于此模型，对给定的输入X，利用贝叶斯定理求出后验概率最大的输出Y。
$$
P(A|B) = \frac{{P(B|A)P(A)}}{{P(B)}}
$$
P(A)是A的先验概率。

P(A|B)，A的后验概率

P(B|A)，B的后验概率

P(B)，是B的先验概率，标准化常量。





## 决策树

### 信息熵

信息熵描述的是事件在结果出来之前对可能产生的信息量的期望，描述的是不确定性。**信息熵越大，不确定性越大。**H(D)的值越小，则D的纯度越高。
$$
H(D) =  - \sum\limits_{k = 1}^N {{p_k}lo{g_2}{p_k}} 
$$
其中N为分类数目，

用决策树分类：从根节点开始，对实例的某一特征进行测试，根据测试结果将实例分配到其子节点，此时每个子节点对应着该特征的一个取值，如此递归的对实例进行测试并分配，直到到达叶节点，最后将实例分到叶节点的类中。

**决策树学习的目标：**根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。

决策树学习的本质：从训练集中归纳出一组分类规则，或者说是由训练数据集估计条件概率模型。

**决策树学习的损失函数：**正则化的极大似然函数。

**决策树学习的测试：**最小化损失函数

**决策树学习的目标：**在损失函数的意义下，选择最优决策树的问题。



### 经验熵

当熵中的概率由数据估计（特别是最大似然估计）的到时，所对应的熵称为经验熵（emprircal entropy）。
$$
H(D) =  - \sum\limits_{k = 1}^N {\frac{{|{c_{\rm{k}}}|}}{{|D|}}lo{g_2}\frac{{|{c_{\rm{k}}}|}}{{|D|}}}
$$


### 条件熵

在已知随机变量X的条件下随机变量Y的不确定性，随机变量X给定的条件下随机变量Y的条件熵H(Y|X)，定义X给定条件下Y的条件概率分布的熵对X的数学期望：
$$
H(Y|X) =  - \sum\limits_{k = 1}^N {{p_k}H(Y|x = {x_k})}
$$


### 信息增益

信息增益相对于特征而言的。特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即：
$$
g(D,A) = H(D) - H(D|A)
$$

### 信息增益比

特征A对训练数据集D的信息增益比定义为其信息增益g(D,A)与训练数据集D的经验熵之比：
$$
{g_R}(D,A) = \frac{{g(D,A)}}{{H(D)}}
$$

### 决策树的构建

从数据集构造决策树算法所需的子功能模块工作原理如下：得到原始数据集，然后基于最好的属性值划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分，第一次划分之后，数据将被向下传递到树分支的下一个节点，在此节点在此划分数据，因此可以使用递归的原则处理数据集。



#### ID3算法

核心是在决策树各个节点上对应信息增益准则选择特征，递归地构建决策树。

**具体方法**

1. 从根结点(root node)开始，对结点计算所有可能的特征的信息增益，**选择信息增益最大的特征**作为结点的特征。
2. 由该特征的不同取值建立子节点，再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止；
3. 最后得到一个决策树。



#### C4.5算法

与ID3算法相似，**将信息增益比作为选择特征的标准**。



### 决策树的剪枝

产生的树往往对训练数据的分类很准确，但对未知测试数据的分类缺没有那么精确，即会出现过拟合现象。过拟合产生的原因在于学习时过多的考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树，解决方法是考虑决策树的复杂度，对已经生成的树进行简化。

剪枝：从已经生成的树上裁掉一些子树或叶节点，并将其根节点或父节点作为新的叶子节点，从而简化分类模型。

实现方式：极小化决策树整体的损失函数或代价函数来实现。



# 无监督学习（unsupervised learning）

直接对数据进行建模。没有给定事先标记过的训练范例，所用的数据没有属性或标签这一概念。事先不知道输入数据对应的输出结果是什么。

自动对输入的资料进行分类或分群，以寻找数据的模型和规律。



## Clustering

识别同质组（簇），从原始

